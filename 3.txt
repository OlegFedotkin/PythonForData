1. Для чего о в каких случаях полезны различные варианты усреднения 
для метрик качества классификации: micro, macro, weighted?

Микро-усреднение используется, когда необходимо одинаково взвешивать каждый экземпляр или прогноз,
при дисбалансе классов.

Макро-усреднение используется, когда все классы должны обрабатываться одинаково, 
чтобы можно было оценить общую производительность классификатора в отношении наиболее частых меток классов.

Взвешенное макро-усреднение используется в случае дисбаланса классов (разное количество экземпляров,
связанных с разными метками классов).




2. В чем разница между моделями xgboost, lightgbm, catboost или какие их основные особенности?


Разделение классов

LightGBM использует одностороннюю выборку на основе градиента, которая выбирает разделение, 
используя все экземпляры с большими градиентами и случайную выборку экземпляров 
с небольшими градиентами. Чтобы сохранить одинаковое распределение данных при вычислении прироста 
информации, градиент вводит постоянный множитель для экземпляров данных с небольшими градиентами. 
Таким образом, градиент  обеспечивает хороший баланс между увеличением скорости за счет сокращения числа 
экземпляров данных и сохранением точности для изученных деревьев решений. 

Catboost использует выборку с минимальной дисперсией, которая представляет собой взвешенную версию 
стохастического градиентного бустинга. В этом методе взвешенная выборка происходит на уровне дерева, 
а не на уровне разделения. Наблюдения для каждого дерева бустинга отбираются таким образом, 
чтобы максимизировать точность разделения оценок.

XGBoost не использует никаких методов взвешенной выборки, что делает его процесс разделения 
медленнее по сравнению LightGBM и Catboost.



Рост листьев


Catboost выращивает сбалансированное дерево. На каждом уровне такого дерева выбирается пара 
разделения функций, которая приводит к наименьшим потерям (в соответствии с функцией штрафа), 
и используется для всех узлов уровня.

LightGBM использует рост деревьев по листьям (лучший первый). Он выбирает выращивать лист, 
который минимизирует потери, позволяя расти несбалансированному дереву. Поскольку он растет 
не по уровням, а по листьям, переоснащение может произойти, когда данные малы. 
В этих случаях важно контролировать глубину дерева.


XGBoost разбивается до указанного гиперпараметра max_depth, а затем начинает обрезку дерева в обратном направлении и удаляет разбиения, 
за пределами которых нет положительного выигрыша. Он использует этот подход, поскольку иногда за 
разделением без уменьшения потерь может последовать разделение с уменьшением потерь. 



Обработка пропущенных значений

Catboost имеет два режима обработки пропущенных значений: “Min” и “Max”. 
В “Min” пропущенные значения обрабатываются как минимальное значение для объекта 
(им присваивается значение, меньшее, чем все существующие значения). 



LightGBM и XGBoost недостающие значения будут распределены на сторону, которая уменьшает потери при каждом разделении.


Важность признаков

Catboost имеет два метода: первый - “PredictionValuesChange". Для каждой функции PredictionValuesChange
показывает, насколько в среднем изменяется прогноз, если изменяется значение функции. 
Функция будет иметь большее значение, когда изменение значения функции приведет к 
значительному изменению прогнозируемого значения.
Второй метод - “LossFunctionChange". Этот тип важности признаков может использоваться для любой модели, но особенно полезен для 
ранжирования моделей. Для каждой функции значение представляет собой разницу между значением 
потерь модели с этой функцией и без нее. Поскольку переобучение модели без одной из функций требует 
больших вычислительных затрат, эта модель строится приблизительно с использованием исходной модели, 
при этом эта функция удаляется из всех деревьев в ансамбле. 

LightGBM и XGBoost имеют два похожих метода: первый-это “Усиление”, которое представляет собой 
повышение точности (или общего усиления), вносимое функцией в ветви, на которых она находится. 
Второй метод имеет другое название в каждом пакете: “split” (LightGBM) и “Frequency”/”Weight” (XGBoost).
Этот метод вычисляет относительное количество раз, когда конкретный объект встречается во всех 
разбиениях деревьев модели. Этот метод может быть предвзят категориальными признаками с большим 
количеством категорий.

У XGBoost есть еще один метод, “Охват”, который представляет собой относительное 
количество наблюдений, связанных с объектом. Для каждого объекта подсчитывается количество наблюдений,
используемых для определения конечного узла.



Обработка категориальных признаков


Catboost использует комбинацию однократного кодирования и расширенного среднего кодирования. 
Для функций с небольшим количеством категорий он использует однократное кодирование. 
Максимальное количество категорий для однократного кодирования может управляться параметром 
one_hot_max_size. Для остальных категориальных столбцов CatBoost использует эффективный метод 
кодирования, который аналогичен среднему кодированию, но с дополнительным механизмом, 
направленным на уменьшение перенапряжения. Использование категориального кодирования CatBoost 
имеет недостаток более медленной модели.

LightGBM разбивает категориальные объекты, разделяя их категории на 2 подмножества. 
Основная идея состоит в том, чтобы отсортировать категории в соответствии с целью обучения на каждом 
этапе. Он имеет сопоставимую (а иногда и худшую) производительность, чем другие методы 
(например, кодирование цели или метки).


XGBoost не имеет встроенного метода для категориальных функций.



